{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"trofi-x_bert_full.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kImUxL3RupA_","executionInfo":{"status":"ok","timestamp":1635786064647,"user_tz":-60,"elapsed":8705,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"196e37ef-25ea-4764-ccf2-bdbd63bac13d"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AiqqEN08lW7J","executionInfo":{"status":"ok","timestamp":1635786074863,"user_tz":-60,"elapsed":4908,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"5e878014-622d-494c-f1df-388a39f6747e"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.26.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJkqUiNeDpoC","executionInfo":{"status":"ok","timestamp":1635786083160,"user_tz":-60,"elapsed":8306,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"85e39a86-54bc-4960-e393-cce45f0f87df"},"source":["# install the full version\n","!pip install pycaret[full]"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pycaret[full] in /usr/local/lib/python3.7/dist-packages (2.3.4)\n","Requirement already satisfied: lightgbm>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (3.3.1)\n","Requirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (3.6.0)\n","Requirement already satisfied: plotly>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (5.3.1)\n","Requirement already satisfied: mlxtend>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.19.0)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (7.29.0)\n","Requirement already satisfied: yellowbrick>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.3.post1)\n","Requirement already satisfied: pandas-profiling>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (3.1.0)\n","Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.5.2)\n","Requirement already satisfied: scipy<=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.5.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (3.2.2)\n","Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (3.2.2)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (7.6.5)\n","Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.19.5)\n","Requirement already satisfied: scikit-plot in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.3.7)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.11.2)\n","Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.23.2)\n","Requirement already satisfied: imbalanced-learn==0.7.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.7.0)\n","Requirement already satisfied: spacy<2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (2.2.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.1.5)\n","Requirement already satisfied: Boruta in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.0.1)\n","Requirement already satisfied: cufflinks>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.17.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (3.2.5)\n","Requirement already satisfied: mlflow in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.21.0)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.15.3)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.5.0)\n","Requirement already satisfied: kmodes>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.11.1)\n","Requirement already satisfied: numba<0.54 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.51.2)\n","Requirement already satisfied: pyod in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.9.5)\n","Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (12.9.0)\n","Requirement already satisfied: ray[tune]>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.7.0)\n","Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.18.1)\n","Requirement already satisfied: catboost>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.0.1)\n","Requirement already satisfied: optuna>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (2.10.0)\n","Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.1.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (5.8.0)\n","Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.5.0)\n","Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.9.0)\n","Requirement already satisfied: interpret<=0.2.4 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.2.4)\n","Requirement already satisfied: tune-sklearn>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.4.1)\n","Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (0.40.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pycaret[full]) (1.19.7)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->pycaret[full]) (3.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost>=0.23.2->pycaret[full]) (1.15.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost>=0.23.2->pycaret[full]) (0.10.1)\n","Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret[full]) (0.3.0)\n","Requirement already satisfied: setuptools>=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret[full]) (57.4.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->pycaret[full]) (5.2.1)\n","Requirement already satisfied: interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from interpret<=0.2.4->pycaret[full]) (0.2.7)\n","Requirement already satisfied: skope-rules>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.0.1)\n","Requirement already satisfied: lime>=0.1.1.33 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.2.0.1)\n","Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.3.4)\n","Requirement already satisfied: SALib>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.4.5)\n","Requirement already satisfied: treeinterpreter>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.2.3)\n","Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (21.8.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.26.0)\n","Requirement already satisfied: dash>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.0.0)\n","Requirement already satisfied: dash-cytoscape>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.3.0)\n","Requirement already satisfied: dash-table>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (5.0.0)\n","Requirement already satisfied: ipykernel>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (6.5.0)\n","Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.0.0)\n","Requirement already satisfied: flask-compress in /usr/local/lib/python3.7/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.10.1)\n","Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.1.4)\n","Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.0.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (7.1.2)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.0.1)\n","Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (4.5.0)\n","Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.1.2)\n","Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (5.4.0)\n","Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (5.1.1)\n","Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.12.3)\n","Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (4.8.1)\n","Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (5.3.5)\n","Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.1.3)\n","Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.0.0)\n","Requirement already satisfied: traitlets<6.0,>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (3.7.4.3)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret[full]) (0.18.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret[full]) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret[full]) (3.0.21)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret[full]) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret[full]) (2.6.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret[full]) (0.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret[full]) (4.4.2)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret[full]) (1.0.2)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret[full]) (5.1.3)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret[full]) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret[full]) (3.5.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->IPython->pycaret[full]) (0.8.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.0.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (22.3.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.8.2)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (4.8.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.3.1->pycaret[full]) (0.37.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (4.62.3)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.16.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret[full]) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret[full]) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret[full]) (2.4.7)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret[full]) (2.6.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<0.54->pycaret[full]) (0.34.0)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=2.2.0->pycaret[full]) (1.4.25)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna>=2.2.0->pycaret[full]) (6.5.0)\n","Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna>=2.2.0->pycaret[full]) (0.8.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna>=2.2.0->pycaret[full]) (6.0)\n","Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna>=2.2.0->pycaret[full]) (1.4.1)\n","Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna>=2.2.0->pycaret[full]) (3.9.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=2.2.0->pycaret[full]) (21.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pycaret[full]) (2018.9)\n","Requirement already satisfied: phik>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.12.0)\n","Requirement already satisfied: htmlmin>=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.1.12)\n","Requirement already satisfied: visions[type_image_path]==0.7.4 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.7.4)\n","Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.5.0)\n","Requirement already satisfied: multimethod>=1.4 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret[full]) (1.6)\n","Requirement already satisfied: tangled-up-in-unicode==0.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.1.0)\n","Requirement already satisfied: pydantic>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret[full]) (1.8.2)\n","Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret[full]) (21.2.0)\n","Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret[full]) (2.6.3)\n","Requirement already satisfied: imagehash in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret[full]) (4.2.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret[full]) (7.1.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->IPython->pycaret[full]) (0.7.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.4.1->pycaret[full]) (8.0.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->pycaret[full]) (0.2.5)\n","Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[tune]>=1.0.0->pycaret[full]) (1.41.0)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]>=1.0.0->pycaret[full]) (1.0.2)\n","Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]>=1.0.0->pycaret[full]) (3.5.3)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]>=1.0.0->pycaret[full]) (3.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]>=1.0.0->pycaret[full]) (3.3.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]>=1.0.0->pycaret[full]) (0.8.9)\n","Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.7/dist-packages (from ray[tune]>=1.0.0->pycaret[full]) (2.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.0.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.26.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2021.5.30)\n","Requirement already satisfied: pathos in /usr/local/lib/python3.7/dist-packages (from SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.2.8)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.1.1)\n","Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize>=0.8.1->pycaret[full]) (21.10.1)\n","Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap->pycaret[full]) (0.0.7)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->pycaret[full]) (1.3.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (0.8.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (2.0.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (3.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret[full]) (1.0.0)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (5.6.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.12.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (1.8.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=2.2.0->pycaret[full]) (1.1.5)\n","Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=2.2.0->pycaret[full]) (1.0.4)\n","Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.7/dist-packages (from azure-storage-blob->pycaret[full]) (0.6.21)\n","Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from azure-storage-blob->pycaret[full]) (35.0.0)\n","Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from azure-storage-blob->pycaret[full]) (1.19.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob->pycaret[full]) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob->pycaret[full]) (2.20)\n","Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-storage-blob->pycaret[full]) (1.3.0)\n","Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-storage-blob->pycaret[full]) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-storage-blob->pycaret[full]) (3.1.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pycaret[full]) (0.10.0)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pycaret[full]) (0.5.0)\n","Requirement already satisfied: botocore<1.23.0,>=1.22.7 in /usr/local/lib/python3.7/dist-packages (from boto3->pycaret[full]) (1.22.7)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=2.2.0->pycaret[full]) (2.2.1)\n","Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=2.2.0->pycaret[full]) (0.4.0)\n","Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=2.2.0->pycaret[full]) (3.5.0)\n","Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=2.2.0->pycaret[full]) (2.2.0)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=2.2.0->pycaret[full]) (5.6.0)\n","Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=2.2.0->pycaret[full]) (0.4.4)\n","Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=2.2.0->pycaret[full]) (1.8.2)\n","Requirement already satisfied: brotli in /usr/local/lib/python3.7/dist-packages (from flask-compress->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.0.9)\n","Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->pycaret[full]) (1.35.0)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->pycaret[full]) (1.0.3)\n","Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->pycaret[full]) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->pycaret[full]) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->pycaret[full]) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->pycaret[full]) (4.2.4)\n","Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->pycaret[full]) (1.26.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->pycaret[full]) (1.53.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage->pycaret[full]) (0.4.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->pycaret[full]) (0.16.0)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->pycaret[full]) (3.12.0)\n","Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (3.1.24)\n","Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (0.18.5)\n","Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (1.2.4)\n","Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (5.0.3)\n","Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (20.1.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (0.3)\n","Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (0.4.2)\n","Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret[full]) (0.16.2)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=4.0.0->mlflow->pycaret[full]) (1.2.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow->pycaret[full]) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow->pycaret[full]) (5.0.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (1.5.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (4.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.7.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.5.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.5.1)\n","Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/dist-packages (from pathos->SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (1.6.6.4)\n","Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos->SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.70.12.2)\n","Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pathos->SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.4->interpret<=0.2.4->pycaret[full]) (0.3.0)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow->pycaret[full]) (0.11.0)\n","Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret[full]) (1.16)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret[full]) (2.7.3)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod->pycaret[full]) (0.10.2)\n","Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod->pycaret[full]) (0.5.2)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn->pycaret[full]) (0.5.5)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I005uVellW_s","executionInfo":{"status":"ok","timestamp":1635786087451,"user_tz":-60,"elapsed":4302,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"fcaa3f6d-2c8c-4e5f-8e95-08f75d1dada6"},"source":["!pip install pytorch_pretrained_bert pytorch-nlp"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.26.0)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu111)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.7)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n","Requirement already satisfied: botocore<1.23.0,>=1.22.7 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.22.7)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.7->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.7->boto3->pytorch_pretrained_bert) (1.26.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.7->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.5.30)\n"]}]},{"cell_type":"code","metadata":{"id":"7aYKBJsVlW4h"},"source":["import re\n","import sys\n","import itertools\n","import numpy as np\n","import random as rn\n","import matplotlib.pyplot as plt\n","import torch\n","from pytorch_pretrained_bert import BertModel\n","from torch import nn\n","from torchnlp.datasets import imdb_dataset\n","from pytorch_pretrained_bert import BertTokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim import Adam\n","from torch.nn.utils import clip_grad_norm_\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6p-qmbelW3P"},"source":["rn.seed(321)\n","np.random.seed(321)\n","torch.manual_seed(321)\n","torch.cuda.manual_seed(321)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNgxEu0KZD8N"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ZPGlcNylW1-"},"source":["from transformers import *\n","\n","class GlobalBERT:\n","    def __init__(self):\n","        self.model_class, self.tokenizer_class, self.pretrained_weights = (BertModel, BertTokenizer, 'bert-large-cased')\n","\n","        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n","        self.model = self.model_class.from_pretrained(self.pretrained_weights, output_hidden_states=True)\n","\n","    def from_sentence_to_embedding(self, text, use_special_tokens=True):\n","        raise Exception\n","\n","class BERT11Sense(GlobalBERT):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def from_sentence_to_embedding(self, text, use_special_tokens=True):\n","        input_ids = torch.tensor([self.tokenizer.encode(text,\n","                                                   add_special_tokens=use_special_tokens)])\n","        outputs = self.model(input_ids)[2]\n","        return (outputs[12][0]).detach().numpy()\n","       \n","       \n","bb = BERT11Sense()\n","\n","\n","bb.from_sentence_to_embedding(\"hello darling\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WoRN_UxYKux"},"source":["cd drive/My Drive/Colab Notebooks/experiments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1BC9ihNlWy7"},"source":["import pandas as pd\n","df = pd.read_csv('data/trofix.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9F8wtqSlWua"},"source":["df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSF-aRYviBZ2"},"source":["#embeddings = [np.mean(bb.from_sentence_to_embedding(k), axis = 0) for k in df[\"sentence\"].values]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9a93G_FDjjNH"},"source":["#embeddings = np.array(embeddings)\n","#embeddings.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ApQoV8IIYdKZ"},"source":["import sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jyiooyxdm0YI"},"source":["from sklearn.model_selection import train_test_split\n","\n","X = df.sentence.values\n","y = df.label.values\n","\n","X_train, X_val, y_train, y_val =\\\n","    train_test_split(X, y, test_size=0.1, random_state=2020)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDZFVeIF1UTD"},"source":["# Load test data - TroFi\n","import pandas as pd\n","# Use a subset for quick experiments\n","#data = data[:10000]\n","\n","from sklearn.model_selection import train_test_split as tts\n","import pandas as pd\n","data = pd.read_csv(\"data/trofix.csv\")\n","\n","# Split to train, val and test\n","train, test_data = tts(data[[\"sentence\", \"arg1\", \"arg2\", \"verb\", \"label\"]], random_state=42, test_size=0.1)\n","train, val = tts(train, random_state=42, test_size=test_data.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBwVuIbu1Uil"},"source":["import nltk\n","# Uncomment to download \"stopwords\"\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","\n","def text_preprocessing(s):\n","    \"\"\"\n","    - Lowercase the sentence\n","    - Change \"'t\" to \"not\"\n","    - Remove \"@name\"\n","    - Isolate and remove punctuations except \"?\"\n","    - Remove other special characters\n","    - Remove stop words except \"not\" and \"can\"\n","    - Remove trailing whitespace\n","    \"\"\"\n","    s = s.lower()\n","    # Change 't to 'not'\n","    s = re.sub(r\"\\'t\", \" not\", s)\n","    # Remove @name\n","    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n","    # Isolate and remove punctuations except '?'\n","    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n","    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n","    # Remove some special characters\n","    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n","    # Remove stopwords except 'not' and 'can'\n","    s = \" \".join([word for word in s.split()\n","                  if word not in stopwords.words('english')\n","                  or word in ['not', 'can']])\n","    # Remove trailing whitespace\n","    s = re.sub(r'\\s+', ' ', s).strip()\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eElwi_fpuFIj"},"source":["X_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHMcJ_URuI4D"},"source":["y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JB5oz7hQuKzi"},"source":["X_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQkVpqr0uSo0"},"source":["y_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oQL2cBY3AcZ"},"source":["#TF-IDF vectorizer - vectorize text data beforecreating the model\n","\n","%%time\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Preprocess text\n","X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n","X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n","\n","# Calculate TF-IDF\n","tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n","                         binary=True,\n","                         smooth_idf=False)\n","X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n","X_val_tfidf = tf_idf.transform(X_val_preprocessed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vncgSuYX2eDd"},"source":["# Hyperparameter Tuning\n","\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","\n","def get_auc_CV(model):\n","    \"\"\"\n","    Return the average AUC score from cross-validation.\n","    \"\"\"\n","    # Set KFold to shuffle data before the split\n","    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n","\n","    # Get AUC scores\n","    auc = cross_val_score(\n","        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n","\n","    return auc.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jWoRD5I2eYq"},"source":["# MultinomialNB - find the best alpha parameter that gives the highest CV AUC score\n","\n","from sklearn.naive_bayes import MultinomialNB\n","\n","res = pd.Series([get_auc_CV(MultinomialNB(i))\n","                 for i in np.arange(1, 10, 0.1)],\n","                index=np.arange(1, 10, 0.1))\n","\n","best_alpha = np.round(res.idxmax(), 2)\n","print('Best alpha: ', best_alpha)\n","\n","plt.plot(res)\n","plt.title('AUC vs. Alpha')\n","plt.xlabel('Alpha')\n","plt.ylabel('AUC')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fmg-8ouF2efh"},"source":["# Evaluation on validation set - model'saccuracy rate and AUC score on validation set\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n","\n","def evaluate_roc(probs, y_true):\n","    \"\"\"\n","    - Print AUC and accuracy on the test set\n","    - Plot ROC\n","    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n","    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n","    \"\"\"\n","    preds = probs[:, 1]\n","    fpr, tpr, threshold = roc_curve(y_true, preds)\n","    roc_auc = auc(fpr, tpr)\n","    print(f'AUC: {roc_auc:.4f}')\n","       \n","    # Get accuracy over the test set\n","    y_pred = np.where(preds >= 0.5, 1, 0)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    print(f'Accuracy: {accuracy*100:.2f}%')\n","    print(f'Precision: {precision*100:.2f}%')\n","    print(f'Recall: {recall*100:.2f}%')\n","    print(f'F1: {f1*100:.2f}%')\n","    \n","    # Plot ROC AUC\n","    plt.title('Receiver Operating Characteristic')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hYlcsHr2env"},"source":["# Compute predicted probabilities\n","nb_model = MultinomialNB(alpha=1.8)\n","nb_model.fit(X_train_tfidf, y_train)\n","probs = nb_model.predict_proba(X_val_tfidf)\n","\n","# Evaluate the classifier\n","evaluate_roc(probs, y_val)\n","\n","# This is the baseline accuracy score before finetuning"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSvTAJMoSF_k"},"source":["## PyCaret All Models"]},{"cell_type":"code","metadata":{"id":"eV5oY8IMSFcs"},"source":["data = df.sample(frac=0.85, random_state=786)\n","data_unseen = df.drop(data.index)\n","data.reset_index(inplace=True, drop=True)\n","data_unseen.reset_index(inplace=True, drop=True)\n","print('Data for Modeling: ' + str(df.shape))\n","print('Unseen Data For Predictions: ' + str(data_unseen.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6YggRgwKtQB"},"source":["stop_words = stopwords.words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvVWor6MEy0C"},"source":["from pycaret.nlp import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqAJBt_6KMWE"},"source":["%time su_1 = setup(data = df, target = 'label', custom_stopwords=stop_words, session_id=21)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BT2KBCx_KMs2"},"source":["from pycaret.classification import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ts3fTlgwK_Up"},"source":["%time pce_1 = setup(data = df, target = 'label', session_id = 5, train_size = 0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tU2a68tMK_Zi"},"source":["%time compare_models()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCr683B7VoGa"},"source":["### Naive Bayes"]},{"cell_type":"code","metadata":{"id":"iwC1NusyUGQ_"},"source":["nb = create_model('nb')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UH34D-PSUGiq"},"source":["#step2 : model tuning\n","%time tuned_nb = tune_model(nb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9O-MKljUGm9"},"source":["#trained model object is stored in the variable 'dt'. \n","print(tuned_nb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZHbV9NqUjM0"},"source":["#finalized model for deployment \n","final_nb = finalize_model(tuned_nb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLDrlFbIUGsN"},"source":["#prediction on unseen test sample\n","nb_preds = predict_model(final_nb, data=data_unseen)\n","nb_preds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGEV7dg1Ur6J"},"source":["from pycaret.utils import check_metric\n","check_metric(nb_preds['label'], nb_preds['Label'], metric = 'Precision')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sIKDWBxgUx_J"},"source":["from pycaret.utils import check_metric\n","check_metric(nb_preds['label'], nb_preds['Label'], metric = 'Recall')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0Jg0MU8UyI_"},"source":["from pycaret.utils import check_metric\n","check_metric(nb_preds['label'], nb_preds['Label'], metric = 'F1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1N0ryn1wUySY"},"source":["from pycaret.utils import check_metric\n","check_metric(nb_preds['label'], nb_preds['Label'], metric = 'Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTRDvbWRVjpK"},"source":["### Random Forest"]},{"cell_type":"code","metadata":{"id":"SJ8_P_fVMroK"},"source":["rf = create_model('rf')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moxiwkWREy_q"},"source":["#step2 : model tuning\n","%time tuned_rf = tune_model(rf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5T9hvpeEzce"},"source":["#step3 : getting insights from model perfromance\n","#%time evaluate_model(tuned_rf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NcJ0R9u_Ezf9"},"source":["#trained model object is stored in the variable 'dt'. \n","print(tuned_rf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clv_g83wQeNM"},"source":["#finalized model for deployment \n","final_rf = finalize_model(tuned_rf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDB1m7W9Ruyj"},"source":["#prediction on unseen test sample\n","rf_preds = predict_model(final_rf, data=data_unseen)\n","rf_preds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KHoRReRTHw6"},"source":["from pycaret.utils import check_metric\n","check_metric(rf_preds['label'], rf_preds['Label'], metric = 'Precision')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQS7fhuFTH61"},"source":["from pycaret.utils import check_metric\n","check_metric(rf_preds['label'], rf_preds['Label'], metric = 'Recall')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUbpOeZsTIEU"},"source":["from pycaret.utils import check_metric\n","check_metric(rf_preds['label'], rf_preds['Label'], metric = 'F1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cq4ixZV8SXqC"},"source":["from pycaret.utils import check_metric\n","check_metric(rf_preds['label'], rf_preds['Label'], metric = 'Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWpNdknjWbO9"},"source":["### KNN"]},{"cell_type":"code","metadata":{"id":"brSxx_vzN2i9"},"source":["knn = create_model('knn')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bygn7-v2We3P"},"source":["#step2 : model tuning\n","%time tuned_knn = tune_model(knn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g60DII3oWvLp"},"source":["#trained model object is stored in the variable 'dt'. \n","print(tuned_knn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXJCVDhRWxIU"},"source":["#finalized model for deployment \n","final_knn = finalize_model(tuned_knn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZmfGDSRWxNH"},"source":["#prediction on unseen test sample\n","knn_preds = predict_model(final_knn, data=data_unseen)\n","knn_preds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8mtQuCcu2sOO"},"source":["from pycaret.utils import check_metric\n","check_metric(knn_preds['label'], knn_preds['Label'], metric = 'Precision')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jox8kktF2sUP"},"source":["from pycaret.utils import check_metric\n","check_metric(knn_preds['label'], knn_preds['Label'], metric = 'Recall')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lgv5MwxV2sZ4"},"source":["from pycaret.utils import check_metric\n","check_metric(knn_preds['label'], knn_preds['Label'], metric = 'F1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLlS-zPR2suw"},"source":["from pycaret.utils import check_metric\n","check_metric(knn_preds['label'], knn_preds['Label'], metric = 'Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAQYJTx74Cha"},"source":["### Logistic Regression"]},{"cell_type":"code","metadata":{"id":"XRpevGWtN2t-"},"source":["lr = create_model('lr')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHU8V2ZYPPTH"},"source":["tuned_lr = tune_model(lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejRqPHWyPxa_"},"source":["#plot_model(tuned_lr, plot = 'auc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_1rKt84P6zN"},"source":["predict_model(tuned_lr);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U18MVZPc4LkF"},"source":["#trained model object is stored in the variable 'dt'. \n","print(tuned_lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCU6z7YO4LpK"},"source":["#finalized model for deployment \n","final_lr = finalize_model(tuned_lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVDlMBOn4LuM"},"source":["#prediction on unseen test sample\n","lr_preds = predict_model(final_lr, data=data_unseen)\n","lr_preds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUBTrBfY4Lz-"},"source":["from pycaret.utils import check_metric\n","check_metric(lr_preds['label'], lr_preds['Label'], metric = 'Precision')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33ML4SuM4L4W"},"source":["from pycaret.utils import check_metric\n","check_metric(lr_preds['label'], lr_preds['Label'], metric = 'Recall')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVYoGveN4L_h"},"source":["from pycaret.utils import check_metric\n","check_metric(lr_preds['label'], lr_preds['Label'], metric = 'F1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TK264ldJ4MEV"},"source":["from pycaret.utils import check_metric\n","check_metric(lr_preds['label'], lr_preds['Label'], metric = 'Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ek5vpng-5UXz"},"source":["### Support Vector Machine"]},{"cell_type":"code","metadata":{"id":"kEXX1vaMN2yk"},"source":["svm = create_model('svm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBbEhDW6PnbY"},"source":["tuned_svm = tune_model(svm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGqxaAnBN26D"},"source":["predict_model(tuned_svm);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNPMTbf_6Zt8"},"source":["#trained model object is stored in the variable 'dt'. \n","print(tuned_svm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Njd_kBPs6Zy7"},"source":["#finalized model for deployment \n","final_svm = finalize_model(tuned_svm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"daXD4nzf6Z3b"},"source":["#prediction on unseen test sample\n","svm_preds = predict_model(final_svm, data=data_unseen)\n","svm_preds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PY095P496aIB"},"source":["from pycaret.utils import check_metric\n","check_metric(svm_preds['label'], svm_preds['Label'], metric = 'Precision')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bORpL-Sr6aOU"},"source":["from pycaret.utils import check_metric\n","check_metric(svm_preds['label'], svm_preds['Label'], metric = 'Recall')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbPTcRjp6aS6"},"source":["from pycaret.utils import check_metric\n","check_metric(svm_preds['label'], svm_preds['Label'], metric = 'F1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSU6uAe2Ezki"},"source":["from pycaret.utils import check_metric\n","check_metric(svm_preds['label'], svm_preds['Label'], metric = 'Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KfNye5th7pQV"},"source":["### Simple Neural Network - scikit-learn Multi-Layer Perceptron Classifier"]},{"cell_type":"code","metadata":{"id":"3NqIzvTIDNwp"},"source":["#from sklearn.preprocessing import StandardScaler\n","#scaler = StandardScaler()\n","# Fit only to the training data\n","#scaler.fit(X_train)\n","#StandardScaler(copy=True, with_mean=True, with_std=True)\n","# Now apply the transformations to the data:\n","#X_train = scaler.transform(X_train)\n","#X_test = scaler.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YiDyPULW7kP6"},"source":["from sklearn.neural_network import MLPClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sE7LiVsi7kWx"},"source":["mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ct-zepp17keP"},"source":["mlp.fit(X_train_tfidf,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JCWvh2Sf7ki2"},"source":["predictions = mlp.predict(X_val_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cViU02SF7knl"},"source":["from sklearn.metrics import classification_report,confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7aQJLzvC3ol"},"source":["print(confusion_matrix(y_val,predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_G3qKWeC3zY"},"source":["print(classification_report(y_val,predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"caw6m60_Didh"},"source":["##LIME"]},{"cell_type":"code","metadata":{"id":"2b3aMjJjZk0N"},"source":["from sklearn.pipeline import make_pipeline\n","c = make_pipeline(tf_idf, nb_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3e02vV2Zk8J"},"source":["print(c.predict_proba([X_val[0]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcNFqejYcpVs"},"source":["class_names = ['Literal', 'Metaphorical']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gziCIMsiczvD"},"source":["!pip install lime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnRI-yDAZk-u"},"source":["from lime.lime_text import LimeTextExplainer\n","explainer = LimeTextExplainer(class_names=class_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lM6CjM69ZlBY"},"source":["idx = 63\n","exp = explainer.explain_instance(X_val[idx], c.predict_proba, num_features=6)\n","print('Document id: %d' % idx)\n","print('Probability(Metaphorical) =', c.predict_proba([X_val[idx]])[0, 1])\n","print('True class: %s' % class_names[y_val[idx]])\n","exp.show_in_notebook(text=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"140IKaQfc509"},"source":["X_val[63]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UyxgaJCc53Z"},"source":["exp.as_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocVBR8tBeXFN"},"source":["print('Original prediction:', nb_model.predict_proba(X_val_tfidf[idx])[0, 1])\n","tmp = X_val_tfidf[idx].copy()\n","tmp[0, tf_idf.vocabulary_['attacked']] = 0\n","print('Prediction removing some features:', nb_model.predict_proba(tmp)[0, 1])\n","print('Difference:', nb_model.predict_proba(tmp)[0, 1] - nb_model.predict_proba(X_val_tfidf[idx])[0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7i6f8DoCp77t"},"source":["idx = 54\n","exp = explainer.explain_instance(X_val[idx], c.predict_proba, num_features=6)\n","print('Document id: %d' % idx)\n","print('Probability(Metaphorical) =', c.predict_proba([X_val[idx]])[0, 1])\n","print('True class: %s' % class_names[y_val[idx]])\n","exp.show_in_notebook(text=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCtYr3RyqTmv"},"source":["idx = 17\n","exp = explainer.explain_instance(X_val[idx], c.predict_proba, num_features=6)\n","print('Document id: %d' % idx)\n","print('Probability(Metaphorical) =', c.predict_proba([X_val[idx]])[0, 1])\n","print('True class: %s' % class_names[y_val[idx]])\n","exp.show_in_notebook(text=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4AZvJFMeXHb"},"source":["%matplotlib inline\n","fig = exp.as_pyplot_figure()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTYfzCtYeXKn"},"source":["#exp.show_in_notebook(text=False)\n","#exp.save_to_file('/tmp/oi.html')\n","#exp.show_in_notebook(text=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DikCZfaY2ewO"},"source":["# BERT Finetuning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIZA6xaA2e3b"},"source":["# Some processing before Finetuning\n","\n","def text_preprocessing(text):\n","    \"\"\"\n","    - Remove entity mentions (eg. '@united')\n","    - Correct errors (eg. '&amp;' to '&')\n","    @param    text (str): a string to be processed.\n","    @return   text (Str): the processed string.\n","    \"\"\"\n","    # Remove '@name'\n","    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n","\n","    # Replace '&amp;' with '&'\n","    text = re.sub(r'&amp;', '&', text)\n","\n","    # Remove trailing whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0LmCJ0h2e-K"},"source":["# Print sentence 0\n","print('Original: ', X[0])\n","print('Processed: ', text_preprocessing(X[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71ahoYrG4ggH"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') #BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n","\n","# Create a function to tokenize a set of texts\n","def preprocessing_for_bert(data):\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    # Create empty lists to store outputs\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in data:\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","        encoded_sent = tokenizer.encode_plus(\n","            text=text_preprocessing(sent),  # Preprocess sentence\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94rWezAkm0Tq"},"source":["# Concatenate train data and test data\n","all_sentences = df['sentence']\n","\n","# Encode our concatenated data\n","encoded_sentences = [bb.tokenizer.encode(sent, add_special_tokens=True) for sent in all_sentences]\n","\n","# Find the maximum length\n","max_len = max([len(sent) for sent in encoded_sentences])\n","print('Max length: ', max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNJg2p1xm0Ra"},"source":["# Specify `MAX_LEN`\n","MAX_LEN = 64\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train)\n","val_inputs, val_masks = preprocessing_for_bert(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qOObRPum0PM"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wROEEbUIm0M7"},"source":["%%time\n","import torch\n","import torch.nn as nn\n","from transformers import BertModel\n","\n","# Create the BertClassfier class\n","class BertClassifier(nn.Module):\n","    \"\"\"Bert Model for Classification Tasks.\n","    \"\"\"\n","    def __init__(self, freeze_bert=False):\n","        \"\"\"\n","        @param    bert: a BertModel object\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(BertClassifier, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H, D_out = 768, 50, 2\n","\n","        # Instantiate BERT model\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Feed input to BERT and the classifier to compute logits.\n","        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n","                      max_length)\n","        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n","                      information with shape (batch_size, max_length)\n","        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n","                      num_labels)\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WD6joVWYweff"},"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","def initialize_model(epochs=3):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n","    \"\"\"\n","    # Instantiate Bert Classifier\n","    bert_classifier = BertClassifier(freeze_bert=False)\n","\n","    # Tell PyTorch to run the model on GPU\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    optimizer = AdamW(bert_classifier.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Set up the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bN7zjasywepq"},"source":["import random\n","import time\n","\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=10, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\n","    \"\"\"\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After the completion of each training epoch, measure the model's performance\n","            # on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    \n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRbSbwU8Io6t"},"source":["import torch\n","\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDpNC90hwenC"},"source":["set_seed(42)    # Set seed for reproducibility\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=3)\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lPy_AKeBwekx"},"source":["import torch.nn.functional as F\n","\n","def bert_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n","    on the test set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    # For each batch in our test set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","    \n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsRAhnHHI5Fy"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n","\n","def evaluate_roc(probs, y_true):\n","    \"\"\"\n","    - Print AUC and accuracy on the test set\n","    - Plot ROC\n","    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n","    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n","    \"\"\"\n","    preds = probs[:, 1]\n","    fpr, tpr, threshold = roc_curve(y_true, preds)\n","    roc_auc = auc(fpr, tpr)\n","    print(f'AUC: {roc_auc:.4f}')\n","       \n","    # Get accuracy over the test set\n","    y_pred = np.where(preds >= 0.5, 1, 0)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    print(f'Accuracy: {accuracy*100:.2f}%')\n","    \n","    # Plot ROC AUC\n","    plt.title('Receiver Operating Characteristic')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DPjzTEcAi0I0"},"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOd1vpXH8u5J"},"source":["preds = probs[:, 1]\n","y_pred = np.where(preds >= 0.5, 1, 0)\n","\n","#importing confusion matrix\n","from sklearn.metrics import confusion_matrix\n","confusion = confusion_matrix(y_val, y_pred)\n","print('Confusion Matrix\\n')\n","print(confusion)\n","\n","#importing accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_val, y_pred)))\n","\n","print('Micro Precision: {:.2f}'.format(precision_score(y_val, y_pred, average='micro')))\n","print('Micro Recall: {:.2f}'.format(recall_score(y_val, y_pred, average='micro')))\n","print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_val, y_pred, average='micro')))\n","\n","print('Macro Precision: {:.2f}'.format(precision_score(y_val, y_pred, average='macro')))\n","print('Macro Recall: {:.2f}'.format(recall_score(y_val, y_pred, average='macro')))\n","print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_val, y_pred, average='macro')))\n","\n","print('Weighted Precision: {:.2f}'.format(precision_score(y_val, y_pred, average='weighted')))\n","print('Weighted Recall: {:.2f}'.format(recall_score(y_val, y_pred, average='weighted')))\n","print('Weighted F1-score: {:.2f}'.format(f1_score(y_val, y_pred, average='weighted')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0GjT4yOzn6nS"},"source":["def get_predictions(model, data_loader):\n","  model.eval()\n","  \n","  sentence_texts = []\n","  predictions = []\n","  prediction_probs = []\n","  real_values = []\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","\n","      texts = d[\"sentence_text\"]\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      probs = F.softmax(outputs, dim=1)\n","\n","      sentence_texts.extend(texts)\n","      predictions.extend(preds)\n","      prediction_probs.extend(probs)\n","      real_values.extend(targets)\n","\n","  predictions = torch.stack(predictions).cpu()\n","  prediction_probs = torch.stack(prediction_probs).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return sentence_texts, predictions, prediction_probs, real_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p76u26nk-c0D"},"source":["from sklearn.metrics import classification_report\n","print('\\nClassification Report\\n')\n","print(classification_report(y_val, y_pred, target_names=['Literal','Metaphorical']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugV7kWw0V3SW"},"source":["#import torch\n","#torch.save(bert_classifier, 'bert_finetuned.pth')\n","\n","#saved_model = torch.load('bert_finetuned.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuJvs3wbBUWc"},"source":["import seaborn as sns\n","def show_confusion_matrix(confusion_matrix):\n","  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n","  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n","  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n","  plt.ylabel('True')\n","  plt.xlabel('Predicted');\n","\n","cm = confusion_matrix(y_val, y_pred)\n","#df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n","show_confusion_matrix(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQQ6JqjmLM7_"},"source":["##Predictions on Test Set"]},{"cell_type":"code","metadata":{"id":"_ixnHOPGJtlr"},"source":["# Run `preprocessing_for_bert` on the test set\n","print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(test_data.sentence.astype(str))\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OhHo8vwTJtpv"},"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.4\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"Number of sentences predicted non-metaphorical: \", preds.sum())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4H7YHhELNW6a"},"source":["output = pd.DataFrame({'sentence': test_data.sentence,\n","                       'correct_label': test_data.label,\n","                       'arg1': test_data.arg1,\n","                       'arg2': test_data.arg1,\n","                       \"verb\": test_data.verb,\n","                       'prediction': preds})\n","output.to_csv('mohx_submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ka-9VgbYNW-E"},"source":["predictions = pd.read_csv(\"stockholm/bert_code/mohx_bert_subs/trofix_submission.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZFGuiG5Nqul"},"source":["predictions.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MtgblyENqyS"},"source":["predictions.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdQbskI6OE7W"},"source":["test_data.label.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9eOFZUSNq2Z"},"source":["final = predictions[(predictions['correct_label'] == predictions['prediction'])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vn4y_rctNzD2"},"source":["final.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0oDo8BgNzLr"},"source":["correct_met = predictions[(predictions['correct_label'] == 1) & (predictions['prediction'] ==1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bWPuMD1nNzPL"},"source":["correct_met.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QoA59YQNzS6"},"source":["correct_lit = predictions[(predictions['correct_label'] == 0) & (predictions['prediction'] ==0)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKR4oAvYNXCF"},"source":["correct_lit.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BzGy22lnYRrV"},"source":["## Save the Model and Import it\n"]},{"cell_type":"code","metadata":{"id":"p-3Nryb4YUao"},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = 'stockholm/bert_code/mohx_bert/bert_model_save'\n","\n","# output_dir = './content/xlm-roberta_model_save/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = bert_classifier.module if hasattr(bert_classifier, 'module') else bert_classifier  # Take care of distributed/parallel training\n","torch.save(model_to_save, 'bert_improved.pt')\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmvDS1DyYUhh"},"source":["import torch\n","# Load the BERT tokenizer.\n","print('Loading Bert Base Uncased Tokenizer...')\n","bert_tokenizer = BertTokenizerFast.from_pretrained(output_dir)\n","model = torch.load('stockholm/mohx_bert/bert_model_save/bert_improved.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pt0r0wMLfnzf"},"source":["model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQYMTurePmzP"},"source":["## Create the Model"]},{"cell_type":"code","metadata":{"id":"9osdcVFjLvKe"},"source":["# Create the model\n","model_e = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n","# Load pre-trained weights\n","#checkpoint = torch.load(\"saved_weights.pt\", map_location=\"cpu\")\n","# Add them to the model\n","#model_e.load_state_dict(checkpoint)\n","model_e = model_e.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NsVhC0GJX6-"},"source":["#max pooling to generate a fixed sized sentence embedding\n","\n","\n","#Max Pooling - Take the max value over time for every dimension\n","def max_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.resize_(token_embeddings.size())\n","    #input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n","    max_over_time = torch.max(token_embeddings, 1)[0]\n","    return max_over_time\n","\n","def avg_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.resize_(token_embeddings.size())\n","    #input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n","    avg_over_time = torch.mean(token_embeddings, 1)[0]\n","    return avg_over_time\n","\n","\n","#Sentences we want sentence embeddings for\n","sentences = ['The', 'stars', 'gravitate', 'towards', 'each', 'other.']\n","\n","#Tokenize sentences\n","encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","encoded_input = encoded_input.to(device)\n","\n","#Compute token embeddings\n","with torch.no_grad():\n","    model_output = model_e(**encoded_input)\n","\n","#Perform pooling. In this case, max pooling\n","sentence_embeddings = max_pooling(model_output, encoded_input['attention_mask'])\n","#avg_sentence_embeddings = avg_pooling(model_output, encoded_input['attention_mask'])\n","\n","\n","print(\"Sentence embeddings:\")\n","print(sentence_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rKO4y3pJYFk"},"source":["# numpy implementation of argmax\n","from numpy import argmax\n","\n","sentence_embeddings = sentence_embeddings.cpu()\n","\n","# get argmax\n","result = argmax(sentence_embeddings)\n","print('arg max of %s: %d' % (sentence_embeddings, result))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSYkiG3mJYJe"},"source":["#CLS token of each input represents the sentence embedding\n","\n","\n","#Sentences we want sentence embeddings for\n","sentences = ['The', 'stars', 'gravitate', 'towards', 'each', 'other']\n","\n","\n","#Tokenize sentences\n","encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","encoded_input = encoded_input.to(device)\n","\n","#Compute token embeddings\n","with torch.no_grad():\n","    model_output = model_e(**encoded_input)\n","    #model_output = model_output.to(device)\n","    \n","sentence_embeddings = model_output[0][:,0] #Take the first token ([CLS]) from each sentence \n","\n","print(\"Sentence embeddings:\")\n","print(sentence_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1x-sLZ0-MEeH"},"source":["# numpy implementation of argmax\n","from numpy import argmax\n","\n","sentence_embeddings = sentence_embeddings.cpu()\n","\n","# get argmax\n","result = argmax(sentence_embeddings)\n","print('arg max of %s: %d' % (sentence_embeddings, result))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WN8RDJUiM7ba"},"source":["##Attention and ArgMax"]},{"cell_type":"code","metadata":{"id":"1wka-J1tP3P1"},"source":["# Use a subset for quick experiments\n","#subset_data = data[:10000]\n","\n","from sklearn.model_selection import train_test_split as tts\n","\n","# Split to train, val and test\n","train, test = tts(df[[\"sentence\", \"label\"]], random_state=42, test_size=0.1)\n","train, val = tts(train, random_state=42, test_size=test.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7YaUDSLS0bN"},"source":["# Construct a BERT tokenizer based on WordPiece\n","bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFfxFMYBM_vG"},"source":["# A sanity check of the tokenizer\n","encoded_instance = bert_tokenizer.batch_encode_plus([train.iloc[0].sentence], padding=True)\n","print(encoded_instance)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvNi5dmqNAEQ"},"source":["print(\"Original text:\", df.iloc[0].sentence)\n","print(\"BERT BPEs:\", bert_tokenizer.convert_ids_to_tokens(encoded_instance[\"input_ids\"][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BdonmB0gNAHs"},"source":["# Set max_len to the maximum length of the training data \n","max_len = max([len(bert_tokenizer.encode(s)) for s in df.sentence.to_list()])\n","print(\"The maximum sentence length in training based on BERT BPEs is\", max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRpnzP8GNAMF"},"source":["# Tokenize and encode sentences in each set\n","x_train = bert_tokenizer.batch_encode_plus(\n","    train.sentence.tolist(),\n","    max_length = max_len,\n","    padding=True,\n","    truncation=True\n",")\n","x_val = bert_tokenizer.batch_encode_plus(\n","    val.sentence.tolist(),\n","    max_length = max_len,\n","    padding=True,\n","    truncation=True\n",")\n","x_test = bert_tokenizer.batch_encode_plus(\n","    test.sentence.tolist(),\n","    max_length = max_len,\n","    padding=True,\n","    truncation=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSuPaB7_NAQF"},"source":["# Convert lists to tensors in order to feed them to our PyTorch model\n","train_seq = torch.tensor(x_train['input_ids'])\n","train_mask = torch.tensor(x_train['attention_mask'])\n","train_y = torch.tensor(train.label.tolist())\n","\n","val_seq = torch.tensor(x_val['input_ids'])\n","val_mask = torch.tensor(x_val['attention_mask'])\n","val_y = torch.tensor(val.label.tolist())\n","\n","test_seq = torch.tensor(x_test['input_ids'])\n","test_mask = torch.tensor(x_test['attention_mask'])\n","test_y = torch.tensor(test.label.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xsrbw41zNATy"},"source":["batch_size = 32\n","\n","# Create a dataloader for each set\n","\n","# TensorDataset: Creates a PyTorch dataset object to load data from\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","# RandomSampler: specify the sequence of indices/keys used in data loading\n","train_sampler = RandomSampler(train_data)\n","# DataLoader: a Python iterable over a dataset\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","\n","test_data = TensorDataset(test_seq, test_mask, test_y)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZY62S-7JYN2"},"source":["# Get attention heatmaps\n","import matplotlib\n","from IPython.core.display import display, HTML\n","def colorize(words, color_array):\n","    cmap=matplotlib.cm.Reds\n","    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n","    colored_string = ''\n","    for word, color in zip(words, color_array):\n","        color = matplotlib.colors.rgb2hex(cmap(color)[:3])\n","        colored_string += template.format(color, '&nbsp' + word + '&nbsp')\n","    return colored_string"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lX-XCtOjQkAC"},"source":["from scipy.special import softmax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPnyW34fMbEg"},"source":["# Predict for the test set and save the results\n","model_e.eval()\n","test_predictions = []\n","test_targets = []\n","test_attentions = []\n","test_inputs = []\n","\n","for batch in test_dataloader:\n","  batch = [t.to(device) for t in batch]\n","  sent_id, mask, labels = batch\n","  # Get gold labels\n","  test_targets.extend(labels.detach().cpu().numpy())\n","  # Get input words\n","  test_inputs.append(bert_tokenizer.convert_ids_to_tokens(sent_id.detach().cpu().numpy()[0]))\n","  with torch.no_grad():\n","    # Get predictions\n","    outputs = model_e(sent_id, attention_mask=mask)\n","    # Apply softmax to the outputs\n","    output_probs = softmax(outputs.logits.detach().cpu().numpy(), axis=1)\n","    # Get the with the highest probability as the predicted label\n","    test_predictions.extend(np.argmax(output_probs, axis=1))\n","    # Get attention weights\n","    # Attention weights from all layers are returned in a tuple\n","    # The weights from each layer are in a tensor with shape (batch_size, attention_heads, max_len, max_len)\n","    test_attentions.append(outputs.attentions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzMPDA2UJYRb"},"source":["# Max Pooling for all tokens in sentences and argmax\n","\n","# Select some sentences randomly\n","sent_index = [1, 2, 3]\n","\n","for s in sent_index:\n","  print(\"*\" * 100)\n","  # For each layer...\n","  for l in range(12):\n","    print(\"\\nLayer\", l+1)\n","    attention = np.squeeze(test_attentions[s][l].detach().cpu().numpy(), axis=0)\n","    # and for each head\n","    for h, head in enumerate(attention):\n","      print(\"Head\", h+1)\n","      # Get the sentence's words\n","      tokens = test_inputs[s]\n","      encoded_tokens = bert_tokenizer(tokens, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","      encoded_tokens = encoded_tokens.to(device)\n","      with torch.no_grad():\n","        model_output1 = model_e(**encoded_tokens)\n","        tokens_embeddings = max_pooling(model_output1, encoded_tokens['attention_mask'])\n","        tokens_embeddings = tokens_embeddings.cpu()\n","        # Get the attention for the cls token\n","        cls_attentions = head[0]\n","        display(HTML(colorize(tokens, cls_attentions)))\n","        print(\"Tokens embeddings:\")\n","        print(tokens_embeddings)\n","        arg = argmax(tokens_embeddings)\n","        print('arg max of %s: %d' % (tokens_embeddings, arg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZalMAGIXVUNK"},"source":["# Max Pooling for all tokens in sentences and argmax\n","\n","# Select some sentences randomly\n","sent_index = [4, 5, 6]\n","\n","for s in sent_index:\n","  print(\"*\" * 100)\n","  # Get the sentence's words\n","  tokens = test_inputs[s]\n","  # For each layer...\n","  for l in range(12):\n","    print(\"\\nLayer\", l+1)\n","    attention = np.squeeze(test_attentions[s][l].detach().cpu().numpy(), axis=0)\n","    # and for each head\n","    for h, head in enumerate(attention):\n","      print(\"Head\", h+1)\n","      # Get the attention for the cls token\n","      encoded_tokens = bert_tokenizer(tokens, truncation=True, padding=True, max_length=128, return_tensors='pt')\n","      encoded_tokens = encoded_tokens.to(device)\n","      with torch.no_grad():\n","        model_output1 = model_e(**encoded_tokens)\n","        tokens_embeddings = max_pooling(model_output1, encoded_tokens['attention_mask'])\n","        tokens_embeddings = tokens_embeddings.cpu()\n","        cls_attentions = head[0]\n","        display(HTML(colorize(tokens, cls_attentions)))\n","        print(\"Tokens embeddings:\")\n","        print(tokens_embeddings)\n","        arg = argmax(tokens_embeddings)\n","        print('arg max of %s: %d' % (tokens_embeddings, arg))"],"execution_count":null,"outputs":[]}]}