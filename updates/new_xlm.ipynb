{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"new_xlm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VWl6QmtWvScp","executionInfo":{"status":"ok","timestamp":1636718864336,"user_tz":-120,"elapsed":32092,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"92b33f2c-ee43-497b-c18e-82e72f2d0317"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"id":"YhFVPYPNr0pV","executionInfo":{"status":"ok","timestamp":1636718877217,"user_tz":-120,"elapsed":9692,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["%%capture\n","!pip install transformers\n","!pip install sentencepiece"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkMqyrSqkjfY","executionInfo":{"status":"ok","timestamp":1636718896011,"user_tz":-120,"elapsed":15939,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","\n","import transformers\n","from sklearn.metrics import *\n","from transformers import AdamW\n","from tqdm.notebook import tqdm\n","from scipy.special import softmax\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split as tts\n","from transformers import BertTokenizerFast, BertConfig, BertForSequenceClassification, AutoModel\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFxQTRODXFW2","executionInfo":{"status":"ok","timestamp":1636718896012,"user_tz":-120,"elapsed":12,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["# Define the device\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxW73YJsRM-o"},"source":["#### Download"]},{"cell_type":"code","metadata":{"id":"BCs0SRjCx6Ao","executionInfo":{"status":"ok","timestamp":1636719141470,"user_tz":-120,"elapsed":339,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["# pick the path\n","giorgios_path = \"drive/My Drive/Colab Notebooks/experiments/stockholm\"\n","johns_path = \"drive/MyDrive/Resources/stockholm\"\n","path = johns_path # giorgios_path\n","# pick the data source\n","data_source = \"mohx\" #mohx trofi trofix"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"id":"Rg_TXRByxLUW","executionInfo":{"status":"ok","timestamp":1636719143053,"user_tz":-120,"elapsed":1125,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"40f74547-3bd6-4118-c58d-0f23416dc68e"},"source":["data = pd.read_csv(f\"{path}/updates/{data_source}.csv\")\n","\n","#data = pd.read_csv(\"drive/My Drive/Colab Notebooks/experiments/data/trofi.csv\")\n","\n","#the \"mixed\" data sources are the original metaphor datasets + the new data (reconstructed and literal sentences from Wiki and Gutenberg)\n","#for trofi-x, select \"trofix_mixed.csv\"; I already re-did everything with \"moh-x_mixed.csv\", as explained in my previous email\n","print(\"\\nThere are\", len(data), \"sentences\")\n","\n","data[\"label\"] = data[\"label\"].apply(int)\n","data.head(2)"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","There are 647 sentences\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>arg1</th>\n","      <th>arg2</th>\n","      <th>verb</th>\n","      <th>sentence</th>\n","      <th>verb_idx</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>knowledge</td>\n","      <td>NaN</td>\n","      <td>absorb</td>\n","      <td>He absorbed the knowledge or beliefs of his t...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cost</td>\n","      <td>NaN</td>\n","      <td>absorb</td>\n","      <td>He absorbed the costs for the accident .</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        arg1  arg2  ... verb_idx label\n","0  knowledge   NaN  ...        1     1\n","1       cost   NaN  ...        1     1\n","\n","[2 rows x 6 columns]"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"fM-WVYfcRXEv"},"source":["#### Split to training, validation and test\n","\n"]},{"cell_type":"code","metadata":{"id":"CscyHhvhOaJX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636719143831,"user_tz":-120,"elapsed":790,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"a3909298-f0e9-43f4-a85d-3f0b52694cd1"},"source":["# Split to train, val and test\n","train, test = tts(data[[\"sentence\", \"label\"]], random_state=42, test_size=0.1)\n","#\"test_new\" is just an intermediate subset for the split (it was needed to have the \"val\" subset for the following steps), not the real test set for the XLM-R model's predictions\n","#the right test set is \"test\", which corresponds to the original metaphor datasets without the addition of the new data, and it is declared in the next code cell\n","train, val = tts(train, random_state=42, test_size=test.shape[0])\n","before = train.shape[0]\n","# using the mixed trainning set\n","train = pd.read_csv(f\"{path}/updates/{data_source}_train_mixed.csv\")\n","#train = pd.read_csv(\"drive/My Drive/Colab Notebooks/experiments/stockholm/updates/trofi_train_mixed.csv\")\n","after = train.shape[0]\n","print(f\"From {before} to {after} ({100*(after-before)/before:.2f}%)\")"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["From 517 to 945 (82.79%)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"_dAuL8mvdEnL","executionInfo":{"status":"ok","timestamp":1636719143863,"user_tz":-120,"elapsed":59,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"8bc7ca3f-fdf3-4f96-99a4-9e14f0085a72"},"source":["train.head(2)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I ca n't buy this story .</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>European children learn the breast stroke ; th...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  label\n","0                          I ca n't buy this story .      1\n","1  European children learn the breast stroke ; th...      0"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"c1Tio8y2RhCv"},"source":["#### Tokenize and encode with BERT tokenizer"]},{"cell_type":"code","metadata":{"id":"hH6j2siuOree","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636719156871,"user_tz":-120,"elapsed":13065,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"476d3905-b502-4c9d-8e87-03dbda70d3d1"},"source":["# For any data source\n","\n","from transformers import XLMRobertaForSequenceClassification\n","\n","output_dir = f'{path}/xlm_code/mixed_models/{data_source}'\n","#output_dir = \"drive/My Drive/Colab Notebooks/experiments/stockholm/xlm_code/mixed_models/trofi\"\n","\n","print(output_dir)\n","\n","from transformers import XLMRobertaTokenizer\n","import torch\n","# Load the BERT tokenizer.\n","print('Loading XLMRobertaTokenizer...')\n","bert_tokenizer = XLMRobertaTokenizer.from_pretrained(output_dir)\n","model_e = XLMRobertaForSequenceClassification.from_pretrained(output_dir, num_labels = 2, output_attentions = True, output_hidden_states = True,)"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["drive/MyDrive/Resources/stockholm/xlm_code/mixed_models/mohx\n","Loading XLMRobertaTokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at drive/MyDrive/Resources/stockholm/xlm_code/mixed_models/mohx were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKEym4bNGbdB","executionInfo":{"status":"ok","timestamp":1636719156871,"user_tz":-120,"elapsed":31,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"42c7602b-f178-4c96-923d-3a461cdc9e68"},"source":["#max_len = max([len(bert_tokenizer.encode(s)) for s in train.sentence.to_list()])\n","\n","#print(max_len)\n","max_lengths = {\"mohx\":21, \"trofi\":161, \"trofix\":161}\n","MAX_LEN = max_lengths[data_source]\n","\n","encoded_instance = bert_tokenizer.encode_plus(\n","            train.iloc[0].sentence,\n","            truncation = True,                \n","            add_special_tokens = True,\n","            max_length = MAX_LEN,     \n","            pad_to_max_length = True,\n","            return_attention_mask = True,  \n","            return_tensors = 'pt'\n","       )\n","\n","\n","encoded_instance"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    0,    87,   377,   653,    25,    18, 22113,   903, 13765,     6,\n","             5,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hgvt1osXo77","executionInfo":{"status":"ok","timestamp":1636719156872,"user_tz":-120,"elapsed":24,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"87cb8ec3-b04e-4fdc-e6ff-cc94309908c4"},"source":["print(\"Original text:\", train.iloc[0].sentence)\n","print(\"BERT BPEs:\", bert_tokenizer.convert_ids_to_tokens(encoded_instance[\"input_ids\"][0]))"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: I ca n't buy this story .\n","BERT BPEs: ['<s>', '▁I', '▁ca', '▁n', \"'\", 't', '▁buy', '▁this', '▁story', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"]}]},{"cell_type":"code","metadata":{"id":"ZqdDR3Z5sf8r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636719156872,"user_tz":-120,"elapsed":21,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"32151979-5401-4066-e734-b9e98a7fd47b"},"source":["# Set max_len to the maximum length of the training data \n","max_len = max([len(bert_tokenizer.encode(s)) for s in train.sentence.to_list()])\n","print(\"The maximum sentence length in training based on BERT BPEs is\", max_len)"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["The maximum sentence length in training based on BERT BPEs is 161\n"]}]},{"cell_type":"code","metadata":{"id":"widrKAxp53Cc","executionInfo":{"status":"ok","timestamp":1636719157353,"user_tz":-120,"elapsed":499,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["# Tokenize and encode sentences in each set\n","x_train = bert_tokenizer.batch_encode_plus(\n","    train.sentence.tolist(),\n","    max_length = max_len,\n","    padding=True,\n","    truncation=True\n",")\n","x_val = bert_tokenizer.batch_encode_plus(\n","    val.sentence.tolist(),\n","    max_length = max_len,\n","    padding=True,\n","    truncation=True\n",")\n","x_test = bert_tokenizer.batch_encode_plus(\n","    test.sentence.tolist(),\n","    max_length = max_len,\n","    padding=True,\n","    truncation=True\n",")"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCZUd7uzsxSB","executionInfo":{"status":"ok","timestamp":1636719157354,"user_tz":-120,"elapsed":8,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["# Convert lists to tensors\n","train_seq = torch.tensor(x_train['input_ids'])\n","train_mask = torch.tensor(x_train['attention_mask'])\n","train_y = torch.tensor(train.label.tolist())\n","\n","val_seq = torch.tensor(x_val['input_ids'])\n","val_mask = torch.tensor(x_val['attention_mask'])\n","val_y = torch.tensor(val.label.tolist())\n","\n","test_seq = torch.tensor(x_test['input_ids'])\n","test_mask = torch.tensor(x_test['attention_mask'])\n","test_y = torch.tensor(test.label.tolist())"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0kTwfyQtNQO","executionInfo":{"status":"ok","timestamp":1636719157355,"user_tz":-120,"elapsed":8,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["batch_size = 32\n","\n","# Create a dataloader for each set\n","\n","# TensorDataset\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","# RandomSampler\n","train_sampler = RandomSampler(train_data)\n","# DataLoader\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","\n","test_data = TensorDataset(test_seq, test_mask, test_y)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=1)"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_jnk7b-fjkg"},"source":["## Inference"]},{"cell_type":"markdown","metadata":{"id":"w8sZWA-5ErB_"},"source":["#### Load the saved checkpoint"]},{"cell_type":"code","metadata":{"id":"iwnBPyvifi-v","executionInfo":{"status":"ok","timestamp":1636719157655,"user_tz":-120,"elapsed":306,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["model_e = model_e.to(device)"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TTIFpXJGx6y"},"source":["#### Get predictions for test"]},{"cell_type":"code","metadata":{"id":"yoOZqLYeEKDf","executionInfo":{"status":"ok","timestamp":1636719158314,"user_tz":-120,"elapsed":664,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}}},"source":["# Predict for the test set and save the results\n","model_e.eval()\n","test_predictions = []\n","test_targets = []\n","test_attentions = []\n","test_inputs = []\n","\n","for batch in test_dataloader:\n","  batch = [t.to(device) for t in batch]\n","  sent_id, mask, labels = batch\n","  test_targets.extend(labels.detach().cpu().numpy())\n","  test_inputs.append(bert_tokenizer.convert_ids_to_tokens(sent_id.detach().cpu().numpy()[0]))\n","  with torch.no_grad():\n","    # Get predictions\n","    outputs = model_e(sent_id, attention_mask=mask)\n","    output_probs = softmax(outputs.logits.detach().cpu().numpy(), axis=1)\n","    test_predictions.extend(np.argmax(output_probs, axis=1))\n","    test_attentions.append(outputs.attentions)"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bfhinN_UI0Qy"},"source":["#### Evaluate"]},{"cell_type":"code","metadata":{"id":"TybpLeTqIKo3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636719158314,"user_tz":-120,"elapsed":11,"user":{"displayName":"John Pavlopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04627713689373772324"}},"outputId":"0a46e8e9-41c0-4245-d226-c05b230eeb44"},"source":["print(data_source)\n","print(\"F1:\", f1_score(test_targets, test_predictions, average=\"binary\"))\n","print(\"ACC:\", accuracy_score(test_targets, test_predictions))\n","print(\"AUPR:\", average_precision_score(test_targets, test_predictions))\n","print(\"PRECISION:\", precision_score(test_targets, test_predictions))\n","print(\"RECALL:\", recall_score(test_targets, test_predictions))\n","print(\"AUC:\", roc_auc_score(test_targets, test_predictions))"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["mohx\n","F1: 0.8493150684931507\n","ACC: 0.8307692307692308\n","AUPR: 0.7983945483945484\n","PRECISION: 0.8378378378378378\n","RECALL: 0.8611111111111112\n","AUC: 0.8271072796934866\n"]}]},{"cell_type":"code","metadata":{"id":"Q7mVRFJ96PZq"},"source":["{\n","\"mohx\":{\n","    \"F1\": 0.8493150684931507,\n","    \"ACC\": 0.8307692307692308,\n","    \"AUPR\": 0.7983945483945484,\n","    \"PRECISION\": 0.8378378378378378,\n","    \"RECALL\": 0.8611111111111112,\n","    \"AUC\": 0.8271072796934866,\n","},\n","\"trofi\":{\n","    \"F1\": 0.9319727891156463,\n","    \"ACC\": 0.946524064171123,\n","    \"AUPR\": 0.8980252411960712,\n","    \"PRECISION\": 0.9383561643835616,\n","    \"RECALL\": 0.9256756756756757,\n","    \"AUC\": 0.9429263334130591\n","},\n","\"trofix\":{\"F1\": 0.9612403100775193,\n","          \"ACC\": 0.9655172413793104,\n","          \"AUPR\": 0.9447281167108753,\n","          \"PRECISION\": 0.96875,\n","          \"RECALL\": 0.9538461538461539,\n","          \"AUC\": 0.9644230769230769}\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLQv1Sy4taTQ"},"source":[""],"execution_count":null,"outputs":[]}]}